ClusterName=discovery
ControlMachine=slurm.hpc.usc.edu
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=root
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d

SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

#RebootProgram=/etc/slurm/RebootProgram.sh

ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup,task/affinity

#PluginDir=
#FirstJobId=
ReturnToService=0
MaxJobCount=200000
MaxArraySize=5001
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PrologFlags=contain
Prolog=/etc/slurm/prolog
Epilog=/etc/slurm/epilog
#SrunProlog=
#SrunEpilog=
TaskProlog=/etc/slurm/taskprolog
#TaskEpilog=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
UsePAM=0
LaunchParameters=use_interactive_step
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=1200
HealthCheckNodeState=ANY
MailProg=/usr/bin/smail

#
# TIMERS
MessageTimeout=30
SlurmctldTimeout=1200
SlurmdTimeout=1200
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
SchedulerParameters=preemptstrict_order,preempt_reorder_count=3,max_rpc_cnt=160,batch_sched_delay=5,default_queue_depth=1000,bf_continue,bf_max_job_test=3000,bf_window=2880,bf_resolution=600,disable_user_top,bf_max_time=600,bf_job_part_count_reserve=100
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
GresTypes=gpu

PreemptType=preempt/partition_prio
PreemptExemptTime=-1
PreemptMode=CANCEL

#PRIORITY and fairshare

PriorityType=priority/multifactor
PriorityDecayHalfLife=7-0
PriorityFavorSmall=NO

#PriorityUsageResetPeriod=14-0
PriorityWeightFairshare=4000
PriorityWeightAge=7000
PriorityWeightPartition=3000
PriorityWeightJobSize=100
PriorityMaxAge=7-0
PriorityWeightQOS=4000
PropagateResourceLimitsExcept=MEMLOCK
PriorityFlags=FAIR_TREE
##PriorityWeightTRES=CPU=1000,Mem=2000,GRES/gpu=3000
AccountingStorageEnforce=associations,limits,qos,safe

# LOGGING
SlurmctldDebug=verbose
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=fatal
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
#
AccountingStorageTRES=gres/gpu,gres/gpu:a100,gres/gpu:a40,gres/gpu:v100,gres/gpu:p100,gres/gpu:k40
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurm
#AccountingStorageLoc=
#AccountingStoragePass=slurmpass
#AccountingStorageUser=slurm
#
MpiDefault=pmix_v2

# COMPUTE NODES
NodeName=DEFAULT Boards=1 SocketsPerBoard=2 ThreadsPerCore=1 State=UNKNOWN MemSpecLimit=2048

##R6525,epyc-7513 c32 256000
NodeName=b04-[01-08,15-21],b05-[01-08,15-21],b09-[01-08,15-21],b10-[01-08,15-21],b15-[05-22] CoresPerSocket=32 Feature=epyc-7513 RealMemory=256000

##R7525,epyc-7513 c32 256000 gpu:a100:80gb:2
NodeName=b04-[12-14],b05-[12-14],b09-[12-14],b10-[12-14] CoresPerSocket=32 Feature=epyc-7513,a100-80gb RealMemory=256000 Gres=gpu:a100:2

##R7525,epyc-7313 c16 256000 gpu:a40:2
NodeName=b04-[09-11],b05-[09-11],b09-[09-11],b10-[09-11],b11-[09-14] CoresPerSocket=16 Feature=epyc-7313 RealMemory=256000 Gres=gpu:a40:2

##R6525,epyc-7513 c32 256000
NodeName=a01-[02-05,07-09,11-14,16-19],a02-[02-05,07-09,11-14,16-19],a03-[02-05,07-09,11-14,16-19],a04-[02-05,07-09,16-19],b01-[02-04,07-08] CoresPerSocket=32 Feature=epyc-7513 RealMemory=256000

##R6525,epyc-7513 c32 1024000
NodeName=a01-10,a02-10,a03-10,a04-10 CoresPerSocket=32 Feature=epyc-7513 RealMemory=1024000

##R7525,epyc-7513 c32 256000 gpu:a100:40gb:2
NodeName=a01-[01,06,15,20],b01-[01,06,15,20],b02-[01,06,15,20] CoresPerSocket=32 Feature=epyc-7513,a100-40gb RealMemory=256000 Gres=gpu:a100:2

##R7525,epyc-7282 c16 256000 gpu:a40:2
NodeName=a02-[01,06,15,20],a03-[01,06,15,20],a04-[01,06,15,20] CoresPerSocket=16 Feature=epyc-7282 RealMemory=256000 Gres=gpu:a40:2

##R6525,epyc-7542 c32 256000
NodeName=b22-[01-32] CoresPerSocket=32 Feature=epyc-7542 RealMemory=256000

##xl170r,xeon-4116 c12 192800
NodeName=d05-[06-15] CoresPerSocket=12 Feature=xeon-4116 RealMemory=192000
NodeName=d05-[26-42] CoresPerSocket=12 Feature=xeon-4116 RealMemory=192000
NodeName=d06-[15-28] CoresPerSocket=12 Feature=xeon-4116 RealMemory=192000

##c6420,xeon-4116 c12 94800
NodeName=d11-[09-47] CoresPerSocket=12 Feature=xeon-4116 RealMemory=94000

##r740,xeon-6130 c16 191600 gpu:v100:2
NodeName=d11-[02-04] CoresPerSocket=16 Feature=xeon-6130 RealMemory=191000 Gres=gpu:v100:2
NodeName=d13-[02-11] CoresPerSocket=16 Feature=xeon-6130 RealMemory=191000 Gres=gpu:v100:2
NodeName=d14-[03-18] CoresPerSocket=16 Feature=xeon-6130 RealMemory=191000 Gres=gpu:v100:2

##r430,xeon-2540v4 c10 64000
NodeName=d17-[23-37] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=64000
NodeName=d18-[01-35] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=64000

##xl170r,xeon-2640v4 c10 64000
#NodeName=d22-[51-52] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=64000

##xl190r,xeon-2640v4 c10 128400 gpu:p100:2
NodeName=d23-[10,13-16] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=128000 Gres=gpu:p100:2
NodeName=e21-[01-16] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=128000 Gres=gpu:p100:2
NodeName=e22-[01-16] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=128000 Gres=gpu:p100:2
NodeName=e23-[01-02] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=128000 Gres=gpu:p100:2

##nx360m5,xeon-2640v4 C10 63400 (Former Endeavour Shared)
NodeName=e20-[36,38,40,42] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=63400

##nx360m5,xeon-2640v3 C8 63400
#NodeName=e06-[01-04,06-22,24] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400
NodeName=e10-12 CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400
#NodeName=e11-[26-27,29,45,47] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400
#NodeName=e13-[11,26,28-48] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400
NodeName=e15-[10,12,14,16,18,20,22,24] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400
##nx360m5,xeon-2640v3 C8 63400 (Former Endeavour Shared)
NodeName=e14-[41-48],e15-[01-09,11,13,15,17,19,21,23] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400

##nx360m5,xeon-2640v3 C8 63400 gpu:k40:2
#NodeName=e07-[01-16,18] CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400 Gres=gpu:k40:2
#NodeName=e09-18 CoresPerSocket=8 Feature=xeon-2640v3 RealMemory=63400 Gres=gpu:k40:2

##nx360m5,xeon-2640v4 C10 63400 gpu:k40:2 
NodeName=e16-[01-04,06-24] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=63400 Gres=gpu:k40:2
NodeName=e17-[01-04,06,07,09-24] CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=63400 Gres=gpu:k40:2

##nx360m5,xeon-2640v4 C10 63400 (Failed GPU)
NodeName=e16-05 CoresPerSocket=10 Feature=xeon-2640v4 RealMemory=63400

# PARTITIONS
PartitionName=DEFAULT DefaultTime=01:00:00 MaxTime=48:00:00 OverSubscribe=NO State=UP DefMemPerCPU=2048 GraceTime=10 TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu:a100=8.0,GRES/gpu:a40=8.0,GRES/gpu:v100=4.0,GRES/gpu:p100=4.0,GRES/gpu:k40=2.0"

PartitionName=debug Nodes=b11-09,d05-[41-42],e23-02 DefaultTime=01:00:00 MaxTime=01:00:00 PriorityTier=10 QOS=debug
PartitionName=epyc-64 Nodes=a01-[02-05,07-09,11-14,16-19],a02-[02-05,07-09,11-14,16-19],a03-[02-05,07-09,11-14,16-19],a04-[02-05,07-09,16-19],b01-[02-04,07-08],b04-[01-08,15-21],b05-[01-08,15-21],b09-[01-08,15-21],b10-[01-08,15-21],b15-[05-22],b22-[01-32] PriorityTier=1 
PartitionName=main Nodes=d05-[26-40],d06-[15-28],d11-[09-47],d17-[23-37],e14-[41-48],e15-[01-24],e16-[01-24],e17-[01-04,06,07,09-24] Default=YES PriorityTier=1
PartitionName=gpu Nodes=a01-[01,06,15,20],a02-[01,06,15,20],a03-[01,06,15,20],a04-[01,06,15,20],b01-[01,06,15,20],b02-[01,06,15,20],b04-[09-14],b05-[09-14],b09-[09-14],b10-[09-14],b11-[10-14],d11-[02-04],d13-[02-11],d14-[03-18],d23-[10,13-16],e21-[01-16],e22-[01-16],e23-01 PriorityTier=1 QOS=gpu
PartitionName=oneweek Nodes=d05-[06-15],d18-[01-35] MaxTime=168:00:00 PriorityTier=1 QOS=oneweek
PartitionName=largemem Nodes=a01-10,a02-10,a03-10,a04-10 MaxTime=168:00:00 PriorityTier=1 QOS=largemem
PartitionName=htcondor Nodes=e10-12 MaxTime=48:00:00 OverSubscribe=FORCE:50 DefMemPerCPU=1024 PriorityTier=1 QOS=condorsub
